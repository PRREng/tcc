{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def getData():\n",
        "    # Open the .h5 file\n",
        "    with h5py.File(\"data2.h5\", \"r\") as f:\n",
        "        # Access datasets\n",
        "        x_dataset = f[\"x_data\"]\n",
        "        y_dataset = f[\"y_data\"]\n",
        "\n",
        "        # Convert datasets to NumPy arrays\n",
        "        x_np = np.array(x_dataset[:])\n",
        "        y_np = np.array(y_dataset[:])\n",
        "\n",
        "        # Optional: Convert NumPy arrays to PyTorch tensors\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y_np, dtype=torch.long)\n",
        "\n",
        "        x_tensor = x_tensor.permute(0, 2, 1).contiguous()\n",
        "    return x_tensor, y_tensor\n"
      ],
      "metadata": {
        "id": "J2bTtcAcF9uL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class MyDataSet(Dataset):\n",
        "    def __init__(self, features, labels, transform=None,\n",
        "                 target_transform=None):\n",
        "        super(MyDataSet, self).__init__()\n",
        "        # calculate magnitude and phase\n",
        "        real = features[:, :, 0]\n",
        "        imaginary = features[:, :, 1]\n",
        "        magnitude = np.sqrt(real**2 + imaginary**2)\n",
        "        phase = np.arctan2(imaginary, real)\n",
        "\n",
        "        # append magnitude and phase to features\n",
        "        features = np.concatenate((features, magnitude[:, :, None],\n",
        "                                   phase[:, :, None]), axis=2)\n",
        "\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.features[index, :, :]\n",
        "        annotation = self.labels[index]  # there's 5 classes\n",
        "        sample = sample.view(1, 100, 4)\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        if self.target_transform:\n",
        "            annotation = self.target_transform(annotation)\n",
        "        sample = sample.squeeze()\n",
        "\n",
        "        return sample, annotation\n"
      ],
      "metadata": {
        "id": "9QhJJ_N4GPlF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# 5 classes to create\n",
        "\n",
        "\n",
        "# Patchify\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, in_features, d_model):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape -> (B, Seq_len, in_features)\n",
        "        x = self.norm(self.relu(self.linear(x)))\n",
        "        # x shape -> (B, seq_len, d_m)\n",
        "        return x\n",
        "\n",
        "\n",
        "# PositionalEncoding (w/ cls token)\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, seq_len, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model),\n",
        "                                      requires_grad=True)\n",
        "        self.pe = nn.Parameter(torch.randn(1, seq_len + 1, d_model),\n",
        "                               requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape -> (B, seq_len, d_m)\n",
        "        # x = x.squeeze(1)\n",
        "\n",
        "        B, seq_len, _ = x.shape\n",
        "        cls_token = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "        # x shape -> (B, seq_len + 1, d_m)\n",
        "        x = x + self.pe[:, : seq_len + 1]\n",
        "        return x\n",
        "\n",
        "\n",
        "# AttentionHead ( I am not going to be implementing this from scratch)\n",
        "# MultiHeadAttention (Because of performance issues)\n",
        "# Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_d, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout,\n",
        "                                          batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_d),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.LayerNorm(ff_d),\n",
        "            nn.Linear(ff_d, d_model),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape -> (B, seq_len + 1, d_model)\n",
        "        att_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(att_out))\n",
        "        ff_out = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "\n",
        "# TransformerEncoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_d, num_layers, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.encoders = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, num_heads, ff_d, dropout)\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape -> (B, seq_len + 1, d_model)\n",
        "        enc_out = x\n",
        "        for enc in self.encoders:\n",
        "            enc_out = enc(enc_out)\n",
        "        return enc_out\n",
        "\n",
        "\n",
        "# ViT\n",
        "class ViT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        d_model,\n",
        "        seq_len,\n",
        "        num_heads,\n",
        "        ff_d,\n",
        "        num_layers,\n",
        "        num_classes,\n",
        "        dropout,\n",
        "    ):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        self.embedding = Embedding(in_features, d_model)\n",
        "        self.pe = PositionalEncoding(seq_len, d_model)\n",
        "\n",
        "        self.transformer = TransformerEncoder(\n",
        "            d_model, num_heads, ff_d, num_layers, dropout\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_d),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.LayerNorm(ff_d),\n",
        "            nn.Linear(ff_d, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embedding(x)\n",
        "        out = self.pe(embed)\n",
        "        out = self.transformer(out)\n",
        "        # out shape -> (B, seq_len + 1, d_model)\n",
        "        out = self.head(out[:, 0, :]).squeeze()\n",
        "        # out shape -> (B, num_classes)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "L52V7oA_GeRM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Optimizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train_one_epoch(model, data_loader: DataLoader, criterion,\n",
        "                    optimizer: Optimizer, epoch: int, device: str=\"cpu\"):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    avg_acc = 0.0\n",
        "    loop = tqdm(data_loader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\")\n",
        "    for i, batch in enumerate(loop):\n",
        "\n",
        "        x, y = batch[0].to(device), batch[1].to(device)\n",
        "        # forward pass\n",
        "        y_hat = model(x)\n",
        "\n",
        "        # calc loss\n",
        "        loss = criterion(y_hat, y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # zero out the gradient\n",
        "        optimizer.zero_grad()\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(y_hat, dim=-1)\n",
        "        acc = (preds == y).sum().item() / len(y)\n",
        "        avg_acc += acc\n",
        "\n",
        "        loop.set_postfix(loss=total_loss / (i + 1),\n",
        "                         accuracy=100. * avg_acc / (i + 1))\n",
        "\n",
        "    total_loss /= len(data_loader)\n",
        "    avg_acc /= len(data_loader)\n",
        "    return total_loss, avg_acc\n",
        "\n",
        "\n",
        "def test_one_epoch(model, data_loader: DataLoader, criterion,\n",
        "                   epoch: int, device: str=\"cpu\"):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    avg_acc = 0.0\n",
        "    loop = tqdm(data_loader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\")\n",
        "    for i, batch in enumerate(loop):\n",
        "\n",
        "        x, y = batch[0].to(device), batch[1].to(device)\n",
        "        # forward pass\n",
        "        y_hat = model(x)\n",
        "\n",
        "        # calc loss\n",
        "        loss = criterion(y_hat, y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(y_hat, dim=-1)\n",
        "        acc = (preds == y).sum().item() / len(y)\n",
        "        avg_acc += acc\n",
        "\n",
        "        loop.set_postfix(loss=total_loss / (i + 1),\n",
        "                         accuracy=100. * avg_acc / (i + 1))\n",
        "\n",
        "    total_loss /= len(data_loader)\n",
        "    avg_acc /= len(data_loader)\n",
        "    return total_loss, avg_acc\n"
      ],
      "metadata": {
        "id": "SozZVsLRGxDp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CUDA disponível:\", torch.cuda.is_available())\n",
        "print(\"Número de GPUs disponíveis:\", torch.cuda.device_count())\n",
        "print(\"Nome da GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Nenhuma GPU encontrada\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfH4hD5AsRLf",
        "outputId": "77298682-9455-4712-fdf0-1a26524905f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA disponível: True\n",
            "Número de GPUs disponíveis: 1\n",
            "Nome da GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# carregue os dados formatados\n",
        "X, y = getData()\n",
        "# separe os dados\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=24\n",
        ")\n",
        "\n",
        "# crie o dataset\n",
        "train_dataset = MyDataSet(X_train, y_train)\n",
        "test_dataset = MyDataSet(X_test, y_test)\n",
        "\n",
        "# crie o dataloader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
        "print(f\"Train_loader size: {len(train_loader)}\")\n",
        "print(f\"Size of each loader: {len(next(iter(train_loader)))}\")\n",
        "print(f\"Test_loader size: {len(test_loader)}\")\n",
        "\n",
        "# Construct the argument parser\n",
        "model_name = \"vit_r13\"\n",
        "with open(f\"{model_name}.yml\", \"r\") as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "hyperparams = config[\"hyperparams\"]\n",
        "print(hyperparams)\n",
        "torch.cuda.empty_cache()\n",
        "model = ViT(**hyperparams).to(device)\n",
        "# model = CNN()\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total params: {total_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "blbATbd2FteK",
        "outputId": "2f909a9d-ef46-4f7b-c667-bd7aa1406d23"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loader size: 120\n",
            "Size of each loader: 2\n",
            "Test_loader size: 1280\n",
            "{'in_features': 4, 'd_model': 25, 'seq_len': 100, 'num_heads': 5, 'ff_d': 1024, 'num_layers': 7, 'num_classes': 5, 'dropout': 0.1}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-38f7834510b4>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mhyperparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hyperparams\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mViT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# model = CNN()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m     \"\"\"\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCpzVv0mxJqJ",
        "outputId": "35127037-bacf-4880-e55c-5330427cb117"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: torch\n",
            "Version: 2.5.1+cu121\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
            "Required-by: accelerate, fastai, peft, sentence-transformers, timm, torchaudio, torchvision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now load the model params\n",
        "model.load_state_dict(torch.load(f\"{model_name}.pth\", weights_only=True))\n",
        "print(\"loaded model\")\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\",\n",
        "                                                       factor=0.5,patience=20,\n",
        "                                                       min_lr=1e-6)\n",
        "\n",
        "num_epochs = 34\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, criterion, optimizer, epoch, device\n",
        "    )\n",
        "\n",
        "    val_loss, val_acc = test_one_epoch(model, test_loader, criterion,\n",
        "                                       epoch, device)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(val_loss)\n",
        "\n",
        "print(f\"Learning rate {scheduler.get_last_lr()}\")\n",
        "print(f\"Train loss: {train_losses[-1]}, acc: {train_acc}\")\n",
        "print(f\"Val loss: {test_losses[-1]}, acc: {val_acc}\")\n",
        "\n",
        "# plot the loss and val loss\n",
        "plt.plot(range(num_epochs), train_losses, color=\"red\", label=\"train_loss\")\n",
        "plt.plot(range(num_epochs), test_losses, label=\"val_loss\")\n",
        "plt.show()\n",
        "\n",
        "torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "N6hWRjgeTW4b",
        "outputId": "1ced3635-0d0b-4c09-b441-178e04ed9c8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   0%|          | 0/120 [00:00<?, ?batch/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4c4ef9123522>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     train_loss, train_acc = train_one_epoch(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-4-4fb09879bbb6>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, criterion, optimizer, epoch, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# calc loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# zero out the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ]
}